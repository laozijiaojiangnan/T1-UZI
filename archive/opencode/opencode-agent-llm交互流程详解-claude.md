# OpenCode Agent 与 LLM 交互流程详解

> 本文档以一个用户输入需求为起点，详细拆解 OpenCode 完成任务的全过程。每一步都会解释"在做什么"、"为什么这么做"以及"优缺点"。

---

## 目录

1. [概述：从输入到输出的旅程](#概述从输入到输出的旅程)
2. [第一阶段：用户输入的接收](#第一阶段用户输入的接收)
3. [第二阶段：消息的组装与存储](#第二阶段消息的组装与存储)
4. [第三阶段：调用 LLM](#第三阶段调用-llm)
5. [第四阶段：流式响应处理](#第四阶段流式响应处理)
6. [第五阶段：工具调用](#第五阶段工具调用)
7. [第六阶段：多轮循环与任务完成](#第六阶段多轮循环与任务完成)
8. [第七阶段：上下文管理](#第七阶段上下文管理)
9. [完整流程图](#完整流程图)
10. [设计亮点与改进空间](#设计亮点与改进空间)

---

## 概述：从输入到输出的旅程

想象你在终端里输入了一个需求：**"帮我在 src/utils 目录下创建一个日期格式化函数"**

这个简单的需求，背后会经历以下旅程：

```
你的输入 → 前端处理 → 服务器接收 → 消息存储 → LLM思考 → 工具执行 → 结果返回 → 你看到输出
```

整个过程就像一个"接力赛"，每个阶段都有专门的模块负责，确保任务能顺利完成。

---

## 第一阶段：用户输入的接收

### 在做什么？

当你在 OpenCode 的终端界面（TUI）里输入消息并按下回车，会发生以下事情：

```
┌─────────────────────────────────────────────────────────────┐
│  OpenCode TUI (你看到的界面)                                  │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │ > 帮我在 src/utils 目录下创建一个日期格式化函数           │ │
│  └─────────────────────────────────────────────────────────┘ │
│                          ↓ 按下回车                          │
│  1. TUI 捕获你的输入文本                                     │
│  2. 打包成 HTTP 请求                                         │
│  3. 发送到本地服务器                                         │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│  HTTP Server (Hono 框架)                                     │
│  POST /session/{sessionID}/message                          │
│  {                                                          │
│    "parts": [{ "type": "text", "text": "帮我创建..." }],    │
│    "model": { "providerID": "anthropic", "modelID": "..." } │
│  }                                                          │
└─────────────────────────────────────────────────────────────┘
```

### 为什么这么做？

OpenCode 采用了**客户端-服务器分离**的架构：

1. **TUI（终端界面）** 只负责显示和收集输入
2. **Server（服务器）** 负责所有业务逻辑

这就像餐厅里的"前台"和"厨房"的关系——前台负责接待客人、记录点单，厨房负责做菜。

### 优点

| 优点       | 说明                              |
| -------- | ------------------------------- |
| **多端支持** | 同一个服务器可以服务 TUI、Web UI、桌面应用等多种前端 |
| **远程控制** | 你可以从手机访问本地运行的 OpenCode          |
| **职责清晰** | 界面和逻辑分离，更易维护                    |

### 缺点

| 缺点           | 说明                  |
| ------------ | ------------------- |
| **复杂度增加**    | 需要处理 HTTP 通信、序列化等   |
| **启动开销**     | 需要同时启动前端和服务器        |
| **单机使用略显冗余** | 对于只在本地使用的场景，架构有点"重" |

---

## 第二阶段：消息的组装与存储

### 在做什么？

服务器收到你的请求后，需要把它变成一个结构化的"消息对象"，然后存储起来。

```typescript
// 简化版的消息结构
{
  id: "msg_20240115_001",        // 唯一标识
  sessionID: "session_abc123",   // 所属会话
  role: "user",                  // 角色：用户
  agent: "build",                // 使用的 Agent
  model: {
    providerID: "anthropic",     // 提供商
    modelID: "claude-sonnet-4"   // 模型
  },
  time: { created: 1705312345 }, // 创建时间
  parts: [                       // 消息内容（可以有多个部分）
    { type: "text", text: "帮我在 src/utils 目录下创建..." }
  ]
}
```

这个过程还会做一些"智能处理"：

1. **识别文件引用**：如果你输入 `@src/utils/helper.ts`，会自动读取文件内容
2. **识别 Agent 引用**：如果你输入 `@plan`，会切换到 plan Agent
3. **权限映射**：根据当前 Agent 设置工具权限

### 为什么这么做？

把消息存储起来有几个重要原因：

1. **对话历史**：LLM 需要看到之前的对话才能理解上下文
2. **可恢复性**：如果程序崩溃，可以从上次中断处继续
3. **可分享**：可以把会话分享给别人看

### 存储结构（简化版）

```
存储路径                          存储内容
─────────────────────────────────────────────────
session/project_1/sess_abc123  →  会话元信息
message/sess_abc123/msg_001    →  消息元信息
part/msg_001/part_001          →  消息内容（文本/文件/工具调用等）
```

### 优点

| 优点       | 说明                |
| -------- | ----------------- |
| **持久化**  | 关闭程序后对话不丢失        |
| **结构化**  | 便于查询、统计、分析        |
| **增量存储** | 每个"部分"单独存储，支持流式更新 |

### 缺点

| 缺点         | 说明            |
| ---------- | ------------- |
| **I/O 开销** | 频繁的文件读写可能影响性能 |
| **存储空间**   | 长对话会占用较多磁盘空间  |

---

## 第三阶段：调用 LLM

### 在做什么？

这是核心步骤——把你的问题发送给 AI 大模型。OpenCode 使用了 **Vercel AI SDK** 作为统一的调用层。

```
┌─────────────────────────────────────────────────────────────┐
│  准备调用 LLM                                               │
├─────────────────────────────────────────────────────────────┤
│  1. 获取 Provider 配置                                      │
│     └─ 读取 API Key、模型参数等                              │
│                                                             │
│  2. 构建系统提示 (System Prompt)                            │
│     ├─ 环境信息："你在 macOS 上，当前目录是 /project..."      │
│     ├─ Agent 指令："你是一个编程助手，可以使用以下工具..."     │
│     └─ 用户自定义指令（如果有）                               │
│                                                             │
│  3. 收集对话历史                                             │
│     └─ 之前的用户消息和助手回复                              │
│                                                             │
│  4. 注册可用工具                                             │
│     └─ bash, read, edit, glob, grep 等                      │
│                                                             │
│  5. 发起流式调用                                             │
│     └─ streamText({ model, messages, tools, ... })          │
└─────────────────────────────────────────────────────────────┘
```

### 为什么这么做？

**为什么用流式调用（streaming）？**

想象两种场景：
- **非流式**：你等了 30 秒，然后 "砰" 一下，所有文字同时出现
- **流式**：文字像打字机一样逐字出现，你可以边看边等

流式调用让体验更好，因为：
1. 用户能立即看到反馈，不会觉得程序"卡住了"
2. 如果发现 AI 跑偏了，可以提前中断

**为什么要抽象 Provider？**

OpenCode 支持 15+ 个 LLM 提供商（Anthropic、OpenAI、Google 等）。通过统一的抽象层：

```typescript
// 不管用哪个提供商，调用方式都一样
const result = await LLM.stream({
  model: { providerID: "anthropic", modelID: "claude-sonnet-4" },
  messages: [...],
  tools: [...]
})
```

### 代码示意

```typescript
// 简化版的 LLM 调用
async function callLLM(input) {
  // 1. 获取模型配置
  const model = Provider.getLanguage(input.model)

  // 2. 构建系统提示
  const system = [
    "你是一个 AI 编程助手...",
    `当前工作目录: ${process.cwd()}`,
    `操作系统: ${os.platform()}`,
    input.agent.prompt  // Agent 特定的指令
  ].join("\n")

  // 3. 调用 Vercel AI SDK
  return streamText({
    model: model,
    system: system,
    messages: input.messages,
    tools: input.tools,
    temperature: 0.7,
    maxTokens: 8192
  })
}
```

### 优点

| 优点 | 说明 |
|------|------|
| **多模型支持** | 轻松切换不同的 AI 提供商 |
| **流式体验** | 实时看到输出，体验更好 |
| **统一接口** | 一套代码适配所有模型 |

### 缺点

| 缺点 | 说明 |
|------|------|
| **模型差异** | 不同模型的能力和格式有差异，需要适配 |
| **依赖第三方库** | 依赖 Vercel AI SDK，有一定的学习成本 |

---

## 第四阶段：流式响应处理

### 在做什么？

LLM 开始返回内容后，OpenCode 需要实时处理这些"碎片"，并把它们组装成完整的响应。

```
LLM 返回的流式事件
─────────────────────────────────────────────────────────
→ text-start     : 文本开始
→ text-delta     : "好"
→ text-delta     : "的"
→ text-delta     : "，"
→ text-delta     : "我"
→ text-delta     : "来"
→ text-delta     : "帮"
→ text-delta     : "你"
→ text-delta     : "创"
→ text-delta     : "建"
→ text-delta     : "..."
→ text-end       : 文本结束
→ tool-call-start: 工具调用开始
→ tool-call      : { tool: "edit", args: { path: "src/utils/date.ts", ... } }
→ tool-result    : { success: true, ... }
→ finish         : 完成
```

每收到一个事件，OpenCode 会：

1. **更新数据库**：把新内容存起来
2. **发送 SSE 事件**：通知前端刷新显示
3. **累积内容**：组装成完整的消息

### 为什么这么做？

**为什么要实时存储？**

每收到一个"碎片"就存储，而不是等全部完成再存，有几个好处：
- 即使程序崩溃，已收到的内容不会丢失
- 前端可以实时显示进度
- 方便调试和追踪

**SSE (Server-Sent Events) 是什么？**

SSE 是一种服务器向客户端推送消息的技术。想象成一个"单向广播"：

```
服务器  ─── SSE 连接 ───→  前端
        持续推送事件        实时更新界面
```

### 事件处理流程

```typescript
// 简化版的流处理
for await (const event of stream.fullStream) {
  switch (event.type) {
    case "text-delta":
      // 收到一小段文字
      currentText += event.text
      await saveToDatabase(currentText)
      await notifyFrontend({ type: "text-update", text: event.text })
      break

    case "tool-call":
      // LLM 想要调用工具
      await saveToolCall(event)
      // 准备执行工具（下一阶段处理）
      break

    case "finish":
      // 流结束
      await finalizeMessage()
      break
  }
}
```

### 优点

| 优点       | 说明                 |
| -------- | ------------------ |
| **实时反馈** | 用户可以看到 AI "思考" 的过程 |
| **可中断**  | 发现问题可以随时停止         |
| **断点续传** | 中断后可以恢复            |

### 缺点

| 缺点 | 说明 |
|------|------|
| **复杂度高** | 需要处理各种边界情况（网络断开、超时等） |
| **状态管理** | 需要跟踪每个"部分"的状态 |

---

## 第五阶段：工具调用

### 在做什么？

这是 AI Agent 最强大的地方——它不只是"说"，还能"做"。

当 LLM 决定需要执行某个操作（比如创建文件），它会发出一个"工具调用"请求：

```
┌─────────────────────────────────────────────────────────────┐
│  LLM 的思考过程                                             │
├─────────────────────────────────────────────────────────────┤
│  "用户想要创建日期格式化函数..."                             │
│  "我需要在 src/utils/date.ts 里写代码..."                   │
│  "让我调用 edit 工具来创建这个文件"                          │
│                                                             │
│  → 发出工具调用请求:                                         │
│    {                                                        │
│      tool: "edit",                                          │
│      args: {                                                │
│        path: "src/utils/date.ts",                           │
│        content: "export function formatDate(date) { ... }" │
│      }                                                      │
│    }                                                        │
└─────────────────────────────────────────────────────────────┘
```

OpenCode 会执行这个工具，然后把结果告诉 LLM：

```
┌─────────────────────────────────────────────────────────────┐
│  工具执行流程                                               │
├─────────────────────────────────────────────────────────────┤
│  1. 权限检查                                                │
│     └─ "edit 工具是否被允许？目标路径是否在允许范围内？"      │
│                                                             │
│  2. 执行工具                                                │
│     └─ 创建/修改文件 src/utils/date.ts                      │
│                                                             │
│  3. 返回结果                                                │
│     └─ { success: true, message: "文件已创建" }             │
│                                                             │
│  4. 告诉 LLM 结果                                           │
│     └─ LLM 可以继续下一步或回复用户                          │
└─────────────────────────────────────────────────────────────┘
```

### 为什么这么做？

**为什么需要工具？**

纯粹的 LLM 只能"说话"，不能真正操作你的电脑。通过工具系统：
- LLM 可以读取你的代码
- 可以修改文件
- 可以执行命令
- 可以搜索网络

这就是 "AI Agent" 和 "AI 聊天机器人" 的本质区别。

**为什么需要权限系统？**

让 AI 操作你的电脑是有风险的。权限系统确保：
- AI 不会删除重要文件
- 不会执行危险命令
- 敏感操作需要你确认

### 权限规则示例

```typescript
// Agent 的权限配置
const buildAgent = {
  name: "build",
  permission: {
    edit: {
      "*": "allow",                    // 默认允许编辑
      ".env": "deny",                  // 禁止编辑 .env 文件
      "node_modules/**": "deny"        // 禁止编辑 node_modules
    },
    bash: {
      "*": "ask"                       // 执行命令需要确认
    },
    read: {
      "*": "allow",                    // 允许读取所有文件
      ".env": "deny"                   // 但不能读 .env
    }
  }
}
```

### OpenCode 内置工具一览

| 工具 | 功能 | 风险等级 | 默认权限 |
|------|------|----------|----------|
| `read` | 读取文件内容 | 低 | 允许（.env 除外） |
| `glob` | 查找匹配的文件 | 低 | 允许 |
| `grep` | 搜索文件内容 | 低 | 允许 |
| `ls` | 列出目录内容 | 低 | 允许 |
| `edit` | 编辑/创建文件 | 中 | 允许（部分路径除外） |
| `bash` | 执行 Shell 命令 | **高** | 需确认 |
| `task` | 创建子任务 | 中 | 允许 |
| `websearch` | 搜索网络 | 低 | 允许 |
| `question` | 向用户提问 | 低 | 允许 |

### 优点

| 优点 | 说明 |
|------|------|
| **真正的自动化** | AI 可以直接完成任务，而不只是告诉你怎么做 |
| **安全可控** | 权限系统防止危险操作 |
| **可扩展** | 通过 MCP 协议可以添加更多工具 |

### 缺点

| 缺点 | 说明 |
|------|------|
| **有风险** | 即使有权限系统，AI 也可能犯错 |
| **调试困难** | 工具执行出错时不容易定位问题 |
| **依赖 LLM 判断** | LLM 可能选择错误的工具或参数 |

---

## 第六阶段：多轮循环与任务完成

### 在做什么？

一个任务往往不是"一问一答"就能完成的。OpenCode 使用一个**循环**来处理多轮交互：

```
┌─────────────────────────────────────────────────────────────┐
│  主循环 (SessionPrompt.loop)                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  while (任务未完成) {                                        │
│                                                             │
│    1. 获取对话历史                                           │
│       └─ 读取所有之前的消息                                   │
│                                                             │
│    2. 调用 LLM                                              │
│       └─ 发送历史 + 当前问题                                  │
│                                                             │
│    3. 处理响应                                              │
│       ├─ 如果是纯文本 → 显示给用户                           │
│       ├─ 如果是工具调用 → 执行工具，继续循环                   │
│       └─ 如果完成 → 退出循环                                 │
│                                                             │
│    4. 检查退出条件                                           │
│       ├─ LLM 说 "完成了" → 退出                             │
│       ├─ 用户中断 → 退出                                     │
│       ├─ 出错 → 退出                                        │
│       └─ 需要继续 → 下一轮                                   │
│  }                                                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 一个完整的例子

假设你让 AI "创建一个日期格式化函数并写单元测试"：

```
第 1 轮
────────────────────────────────────────
用户: 帮我在 src/utils 创建日期格式化函数，并写单元测试

LLM 思考: 我需要先看看 src/utils 目录结构

LLM 调用: glob({ pattern: "src/utils/**/*" })

工具返回: src/utils/string.ts, src/utils/number.ts

→ 继续下一轮

第 2 轮
────────────────────────────────────────
LLM 思考: 目录存在，我来创建 date.ts 文件

LLM 调用: edit({ path: "src/utils/date.ts", content: "..." })

工具返回: 文件已创建

→ 继续下一轮

第 3 轮
────────────────────────────────────────
LLM 思考: 现在创建测试文件

LLM 调用: edit({ path: "src/utils/__tests__/date.test.ts", content: "..." })

工具返回: 文件已创建

→ 继续下一轮

第 4 轮
────────────────────────────────────────
LLM 思考: 任务完成，告诉用户

LLM 回复: "我已经创建了 date.ts 和对应的测试文件..."

finish_reason: "stop"

→ 退出循环，任务完成
```

### 为什么这么做？

**为什么需要循环？**

复杂任务需要多步操作：
1. 先了解现状（读取文件、查看结构）
2. 制定计划
3. 逐步执行
4. 验证结果

每一步都可能需要 LLM "思考" 和 "行动"。

**退出条件设计**

LLM 返回的 `finish_reason` 告诉我们该怎么做：

| finish_reason | 含义 | 动作 |
|---------------|------|------|
| `stop` | LLM 认为回答完成 | 退出循环 |
| `tool_calls` | LLM 想调用工具 | 执行工具，继续循环 |
| `length` | 输出太长被截断 | 可能需要继续 |

### 防止死循环

有时 LLM 可能会陷入"死循环"——反复调用相同的工具。OpenCode 有检测机制：

```typescript
// 检查最近 3 次调用是否完全相同
if (isDoomLoop(currentCall, lastThreeCalls)) {
  // 暂停并询问用户是否继续
  await askUser("检测到可能的循环，是否继续？")
}
```

### 优点

| 优点         | 说明              |
| ---------- | --------------- |
| **处理复杂任务** | 多轮对话能完成复杂的多步骤任务 |
| **自主性**    | AI 可以自己决定需要多少步  |
| **可中断**    | 用户随时可以介入        |

### 缺点

| 缺点 | 说明 |
|------|------|
| **成本高** | 多轮调用消耗更多 token |
| **可能跑偏** | AI 可能偏离原始目标 |
| **耗时长** | 复杂任务需要较长等待时间 |

---

## 第七阶段：上下文管理

### 在做什么？

LLM 有一个"上下文窗口"限制——它一次只能"看到"有限的内容。当对话太长时，OpenCode 需要做"上下文管理"。

```
┌─────────────────────────────────────────────────────────────┐
│  上下文窗口示意                                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [系统提示] [历史消息1] [历史消息2] ... [当前消息]            │
│  ←────────────── 上下文窗口（如 200K tokens）──────────────→  │
│                                                             │
│  当内容超出窗口时:                                           │
│  [系统提示] [摘要] [最近的消息] [当前消息]                    │
│             ↑                                               │
│             压缩早期的对话                                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 自动压缩机制

当 token 数量接近模型限制时，OpenCode 会自动"压缩"对话：

```
原始对话（太长）:
────────────────────────────────────────
消息1: 用户询问如何重构代码
消息2: AI 分析了 5 个文件
消息3: 用户确认方案
消息4: AI 修改了文件 A
消息5: AI 修改了文件 B
消息6: 用户发现 bug
消息7: AI 修复了 bug
... (继续很多轮)

压缩后:
────────────────────────────────────────
[摘要]: 用户请求代码重构，AI 分析并修改了 5 个文件，
       期间修复了一个 bug。关键改动包括...
消息N-2: 最近的对话
消息N-1: 最近的对话
消息N: 当前消息
```

### 为什么这么做？

**为什么要压缩而不是直接截断？**

直接截断早期消息会丢失重要上下文。通过"智能摘要"：
- 保留关键信息（做了什么、为什么）
- 丢弃不重要的细节（具体的代码片段）
- AI 仍能理解整体背景

### 会话管理功能

| 功能             | 说明              |
| -------------- | --------------- |
| **会话分叉（Fork）** | 从某个点创建分支，尝试不同方案 |
| **会话恢复**       | 关闭后重新打开，继续之前的对话 |
| **会话分享**       | 把对话导出分享给他人      |
| **会话归档**       | 标记为完成，但保留记录     |

### 优点

| 优点 | 说明 |
|------|------|
| **突破上下文限制** | 理论上可以无限对话 |
| **保留核心信息** | 不会完全丢失历史 |
| **节省成本** | 压缩后的上下文更短，token 消耗更少 |

### 缺点

| 缺点 | 说明 |
|------|------|
| **信息损失** | 压缩必然丢失一些细节 |
| **摘要质量** | 摘要的好坏取决于压缩模型 |
| **额外开销** | 压缩本身也需要调用 LLM |

---

## 完整流程图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        OpenCode 完整交互流程                                 │
└─────────────────────────────────────────────────────────────────────────────┘

     用户                    TUI                    Server                  LLM
       │                      │                       │                      │
       │  1. 输入需求         │                       │                      │
       │─────────────────────→│                       │                      │
       │                      │                       │                      │
       │                      │  2. POST /message     │                      │
       │                      │──────────────────────→│                      │
       │                      │                       │                      │
       │                      │                       │  3. 创建 User 消息   │
       │                      │                       │  4. 存储到数据库     │
       │                      │                       │                      │
       │                      │                       │  5. 构建系统提示     │
       │                      │                       │  6. 收集对话历史     │
       │                      │                       │  7. 注册可用工具     │
       │                      │                       │                      │
       │                      │                       │  8. streamText()     │
       │                      │                       │─────────────────────→│
       │                      │                       │                      │
       │                      │                       │  9. 流式响应         │
       │                      │                       │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │
       │                      │                       │                      │
       │                      │  10. SSE 推送        │                      │
       │                      │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│                      │
       │                      │                       │                      │
       │  11. 实时显示        │                       │                      │
       │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │                       │                      │
       │                      │                       │                      │
       │                      │                       │  12. 收到工具调用    │
       │                      │                       │                      │
       │                      │                       │  13. 权限检查        │
       │                      │                       │                      │
       │  14. 权限确认（如需要）│                      │                      │
       │←────────────────────────────────────────────→│                      │
       │                      │                       │                      │
       │                      │                       │  15. 执行工具        │
       │                      │                       │                      │
       │                      │  16. 工具执行状态     │                      │
       │                      │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│                      │
       │                      │                       │                      │
       │  17. 显示执行进度     │                       │                      │
       │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │                       │                      │
       │                      │                       │                      │
       │                      │                       │  18. 返回工具结果    │
       │                      │                       │─────────────────────→│
       │                      │                       │                      │
       │                      │                       │  19. LLM 继续思考    │
       │                      │                       │  （可能更多工具调用） │
       │                      │                       │←─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │
       │                      │                       │                      │
       │                      │                       │  20. 循环步骤 9-19   │
       │                      │                       │                      │
       │                      │                       │  21. 任务完成        │
       │                      │                       │←─────────────────────│
       │                      │                       │                      │
       │                      │  22. 最终响应         │                      │
       │                      │←─────────────────────│                      │
       │                      │                       │                      │
       │  23. 显示完成结果     │                       │                      │
       │←─────────────────────│                       │                      │
       │                      │                       │                      │
       ▼                      ▼                       ▼                      ▼
```

---

## 设计亮点与改进空间

### 设计亮点

| 亮点 | 说明 |
|------|------|
| **流式处理** | 实时反馈，用户体验好 |
| **多 Provider 支持** | 不锁定特定厂商，灵活切换 |
| **权限系统** | 安全可控，防止危险操作 |
| **事件驱动** | 模块间松耦合，易于扩展 |
| **持久化存储** | 可恢复、可追溯、可分享 |
| **MCP 协议** | 标准化的工具扩展方式 |

### 改进空间

| 方面       | 现状                 | 可能的改进      |
| -------- | ------------------ | ---------- |
| **代码组织** | server.ts 约 2500 行 | 拆分为独立路由模块  |
| **错误处理** | 基本的重试机制            | 更智能的错误恢复策略 |
| **性能优化** | 频繁 I/O             | 引入缓存层      |
| **调试工具** | 基本日志               | 更完善的追踪和分析  |
| **测试覆盖** | 部分测试               | 更全面的集成测试   |

---

## 总结

OpenCode 的 Agent-LLM 交互是一个精心设计的系统：

1. **输入接收**：客户端/服务器分离，支持多种前端
2. **消息存储**：结构化存储，支持恢复和分享
3. **LLM 调用**：统一抽象，支持多提供商
4. **流式处理**：实时反馈，用户体验好
5. **工具系统**：让 AI 能真正"做事"，而不只是"说话"
6. **多轮循环**：处理复杂的多步骤任务
7. **上下文管理**：智能压缩，突破上下文限制

理解了这个流程，你就理解了现代 AI Agent 的核心运作原理——这不仅仅是"聊天机器人"，而是一个能够理解需求、规划步骤、执行操作、持续迭代的智能系统。

---

## 参考文件

| 文件 | 作用 |
|------|------|
| `src/session/prompt.ts` | 消息循环核心 |
| `src/session/llm.ts` | LLM 调用层 |
| `src/session/processor.ts` | 流式处理器 |
| `src/server/server.ts` | HTTP 服务器 |
| `src/tool/*.ts` | 各种工具实现 |
| `src/agent/agent.ts` | Agent 定义 |
| `src/provider/provider.ts` | Provider 抽象 |
