# OpenCode Agent 与 LLM 交互流程详解

## 一、流程全景图

当你在 OpenCode 中输入一个需求（比如"帮我写一个 Todo List 组件"）到最终看到结果，整个过程经历了多个精心设计的阶段。下面我用最通俗的方式，带你一步步理解这个过程。

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              用户输入需求                                     │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段一：请求接收与预处理                                                      │
│  - 解析用户输入                                                                │
│  - 确定使用的 Agent                                                            │
│  - 确定使用的 LLM 模型                                                         │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段二：提示词（Prompt）构建                                                  │
│  - 系统提示词                                                                  │
│  - Agent 专用提示词                                                            │
│  - 用户历史消息                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段三：工具（Tool）准备                                                      │
│  - 筛选可用的工具                                                              │
│  - 根据 Agent 权限过滤                                                         │
│  - 转换为 LLM 需要的格式                                                        │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段四：调用 LLM                                                              │
│  - 向 LLM 发送请求                                                             │
│  - 流式接收响应                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段五：处理 LLM 响应                                                         │
│  - 解析文本内容                                                                │
│  - 解析工具调用请求                                                            │
│  - 处理推理过程（reasoning）                                                   │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段六：工具执行与结果处理                                                    │
│  - 执行工具调用                                                                │
│  - 权限检查                                                                    │
│  - 收集工具结果                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  阶段七：循环与终止                                                            │
│  - 将工具结果返回给 LLM                                                        │
│  - LLM 决定是否继续                                                             │
│  - 完成或终止                                                                  │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 二、阶段详解

### 阶段一：请求接收与预处理

#### 发生了什么？

当你输入需求后，OpenCode 首先需要搞清楚几件事：

1. **你用的是什么 Agent**？
   - 默认是 `build` Agent（负责编码）
   - 你可以用 Tab 切换到 `plan` Agent（只读分析）、`explore` Agent（代码探索）等

2. **你准备用哪个 LLM**？
   - 系统会根据配置选择一个默认模型
   - 也可以在配置中指定，比如使用 `anthropic/claude-sonnet-4`

3. **当前会话的状态**？
   - 这是新会话还是继续之前的对话？
   - 之前说了什么？

#### 为什么要这么做？

```
目的：确保后续每一步都有正确的上下文和配置
```

- **Agent 选择**：不同的 Agent 有不同的能力边界。比如 `plan` Agent 被设计为只读，不能修改文件，这保证了代码安全性。
- **模型选择**：不同的模型能力不同、成本不同、支持的特性也不同（比如只有部分模型支持推理过程输出）。

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 灵活性高，可以为不同任务选择最适合的 Agent | 用户需要了解不同 Agent 的区别 |
| 支持多种模型切换 | 初次配置可能复杂 |
| 会话状态持久化，对话可以中断后继续 | 需要存储大量历史数据 |

---

### 阶段二：提示词（Prompt）构建

#### 发生了什么？

这是最关键的步骤之一。OpenCode 会把多种信息拼装成一个大的提示词，发送给 LLM。这个提示词包含三个部分：

```
┌────────────────────────────────────────────────────────────┐
│  系统提示词（System Prompt）                                │
│  - Agent 的专属指令（比如"你是一个专业的前端开发者"）        │
│  - 提供商级别的提示（比如 OpenAI 的默认指导）                │
│  - 插件注入的提示词                                         │
└────────────────────────────────────────────────────────────┘
                              +
┌────────────────────────────────────────────────────────────┐
│  工具描述（Tool Definitions）                               │
│  - 所有可用工具的描述                                       │
│  - 工具的参数格式                                           │
│  - 工具的用途说明                                           │
└────────────────────────────────────────────────────────────┘
                              +
┌────────────────────────────────────────────────────────────┐
│  历史消息（Conversation History）                           │
│  - 之前的对话内容                                           │
│  - 工具调用的记录和结果                                     │
│  - 用户上传的文件信息                                       │
└────────────────────────────────────────────────────────────┘
```

#### 代码层面的理解

```typescript
// 来自 llm.ts
const system = []
system.push(
  [
    // 1. Agent 的专属提示词
    ...(input.agent.prompt ? [input.agent.prompt] : SystemPrompt.provider(input.model)),
    // 2. 任何自定义提示词
    ...input.system,
    // 3. 用户消息中携带的系统提示
    ...(input.user.system ? [input.user.system] : []),
  ].join("\n")
)
```

#### 为什么要这么做？

```
目的：让 LLM 理解它是谁、它能做什么、它需要处理什么
```

- **角色定义**：告诉 LLM 它是一个"编程助手"，这决定了它的回答风格和专业领域。
- **工具告知**：LLM 需要知道它有哪些"工具"可以用，才能在需要时调用它们。
- **上下文提供**：历史对话让 LLM 记得之前说过什么，不会"失忆"。

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 高度可配置，可以通过提示词改变 LLM 行为 | 提示词长度有限制（通常 32K-200K tokens） |
| 历史对话让交互更自然 | 过长的提示词会增加成本和延迟 |
| 支持动态注入提示词（比如插件） | 需要精心设计提示词才能获得好效果 |

#### 实用技巧

如果你想让 LLM 更好地工作，可以在输入中：
1. **明确角色**："你是一个 TypeScript 专家"
2. **提供背景**：说明项目结构、技术栈
3. **给出例子**：展示你期望的代码风格

---

### 阶段三：工具（Tool）准备

#### 发生了什么？

OpenCode 内置了 46 个工具，涵盖了编程的各个方面：

| 类别 | 工具示例 | 用途 |
|------|----------|------|
| 文件操作 | `read`, `write`, `edit`, `glob` | 读取、创建、修改、搜索文件 |
| 命令执行 | `bash`, `shell` | 运行终端命令 |
| 代码分析 | `grep`, `codesearch` | 在代码中搜索特定内容 |
| 版本控制 | `git` | 执行 Git 操作 |
| Web 能力 | `webfetch`, `websearch` | 搜索网络、获取网页 |
| 权限相关 | `permission`, `question` | 权限控制、用户确认 |

但是！**不是所有工具都会给当前的 Agent 使用**。

```typescript
// 来自 llm.ts
async function resolveTools(input) {
  // 根据 Agent 权限禁用某些工具
  const disabled = PermissionNext.disabled(Object.keys(input.tools), input.agent.permission)
  for (const tool of Object.keys(input.tools)) {
    if (input.user.tools?.[tool] === false || disabled.has(tool)) {
      delete input.tools[tool]  // 删除禁用的工具
    }
  }
  return input.tools
}
```

#### 工具注册的示例

```typescript
// 工具通过 @Tool 装饰器注册
@Tool.Define({
    name: "read",
    description: "Read a file from the filesystem",
    parameters: z.object({
        path: z.string(),      // 文件路径
        limit: z.number(),     // 可选：限制行数
        offset: z.number(),    // 可选：起始行号
    }),
})
class ReadTool extends Tool.Handler {
    async run(input: Input) {
        const file = Bun.file(input.path)
        return await file.text()
    }
}
```

#### Agent 权限示例

```typescript
// plan Agent 的权限：拒绝所有编辑操作
const plan = {
  name: "plan",
  permission: PermissionNext.merge(defaults, {
    edit: {
      "*": "deny",  // 拒绝所有编辑
    },
    // 只允许读取 .opencode/plans 下的文件
    read: {
      [path.join(".opencode", "plans", "*.md")]: "allow",
    },
  }),
  mode: "primary",
}
```

#### 为什么要这么做？

```
目的：安全 + 精确
```

- **安全**：你不想让 AI 随意删除文件或者执行危险命令吧？权限系统确保 AI 只能在允许的范围内操作。
- **减少干扰**：如果 Agent 只能读取文件，就不应该告诉它有删除文件的能力，这会让它困惑。

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 安全性高，防止 AI 做危险操作 | 配置复杂，需要理解权限规则 |
| 精确匹配任务需求 | 权限冲突时可能需要仔细调试 |
| 清晰的审计追踪 | 用户需要学习权限语法 |

---

### 阶段四：调用 LLM

#### 发生了什么？

现在，所有准备工作完成，OpenCode 通过 Vercel AI SDK 向 LLM 发送请求：

```typescript
// 来自 llm.ts
return streamText({
  model: language,           // 包装后的语言模型
  messages: [...],           // 构建好的消息
  tools: tools,              // 筛选后的工具
  temperature: params.temperature,  // 温度参数
  maxOutputTokens: 32000,    // 最大输出 tokens
  abortSignal: input.abort,  // 中止信号
  // ... 其他配置
})
```

#### 为什么要用流式（Streaming）？

```typescript
for await (const value of stream.fullStream) {
  switch (value.type) {
    case "text-delta":
      // 实时显示 LLM 的输出
      await Session.updatePart({ part: currentText, delta: value.text })
      break
    case "tool-call":
      // 工具调用请求
      break
  }
}
```

流式传输就像打字机效果：
- LLM 边生成，OpenCode 边显示
- 不用等整个回答完成才看到内容
- 用户体验更好，感知更快

#### Provider 抽象层

OpenCode 支持 20+ 个 LLM 提供商：

```typescript
// 来自 provider.ts
const BUNDLED_PROVIDERS = {
  "@ai-sdk/anthropic": createAnthropic,
  "@ai-sdk/openai": createOpenAI,
  "@ai-sdk/google": createGoogleGenerativeAI,
  "@ai-sdk/azure": createAzure,
  // ... 还有更多
}
```

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 流式传输，用户体验好 | 实现复杂度高 |
| Provider 抽象层支持多模型切换 | 不同模型的 API 有差异，需要适配 |
| 中止信号支持取消请求 | 网络问题可能导致请求失败 |
| 支持重试机制 | 成本因模型而异 |

---

### 阶段五：处理 LLM 响应

#### 发生了什么？

LLM 的响应不是一次性返回的，而是一系列"事件"（Events）。OpenCode 需要处理这些事件：

```typescript
// 来自 processor.ts
for await (const value of stream.fullStream) {
  switch (value.type) {
    // 1. 文本开始
    case "text-start":
      currentText = { type: "text", text: "", ... }
      break

    // 2. 文本增量（边生成边显示）
    case "text-delta":
      currentText.text += value.text
      await Session.updatePart({ part: currentText, delta: value.text })
      break

    // 3. 文本完成
    case "text-end":
      await Session.updatePart(currentText)  // 保存到存储
      break

    // 4. 推理过程开始（某些模型支持）
    case "reasoning-start":
      reasoningMap[value.id] = { type: "reasoning", text: "", ... }
      break

    // 5. 工具调用请求
    case "tool-call":
      const part = await Session.updatePart({
        type: "tool",
        tool: value.toolName,
        callID: value.id,
        state: { status: "running" },
      })
      break
  }
}
```

#### 消息结构设计

```typescript
// 来自 message-v2.ts
const Message = {
  role: "user" | "assistant",
  parts: [
    { type: "text", text: "..." },           // 文本部分
    { type: "tool", tool: "read", ... },     // 工具调用部分
    { type: "reasoning", text: "..." },      // 推理过程部分
    { type: "file", url: "...", ... },       // 文件部分
  ],
}
```

#### 为什么要这样做？

```
目的：细粒度处理响应，支持实时更新
```

- **增量更新**：用户不需要等待完整回答，实时看到生成的内容。
- **多类型处理**：LLM 可能同时输出文本、调用工具、输出推理过程，需要分别处理。
- **状态追踪**：工具调用的状态（pending → running → completed/error）需要清晰追踪。

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 实时显示，用户体验好 | 处理逻辑复杂，需要处理多种事件类型 |
| 支持增量保存，中断后可恢复 | 需要设计良好的状态机 |
| 推理过程可见（某些模型） | 不是所有模型都支持推理输出 |

---

### 阶段六：工具执行与权限检查

#### 发生了什么？

当 LLM 决定调用工具时，OpenCode 需要：

1. **权限检查**：这个 Agent 有这个工具的使用权限吗？
2. **执行工具**：调用对应的工具函数
3. **结果处理**：收集结果，返回给 LLM

```typescript
// 工具执行流程
const DOOM_LOOP_THRESHOLD = 3

if (part.state.status === "running") {
  // 1. 权限检查
  await PermissionNext.check({
    permission: "bash",
    patterns: ["npm install"],
    agent,
  })

  // 2. 防止死循环（Doom Loop）
  const lastThree = parts.slice(-3)
  if (allSameTool(lastThree)) {
    await PermissionNext.ask({
      permission: "doom_loop",
      patterns: [toolName],
    })
  }
}
```

#### 权限检查机制

```typescript
// 权限规则示例
PermissionNext.fromConfig({
  "*": "allow",              // 默认允许
  doom_loop: "ask",          // 循环执行前询问
  read: {
    "*": "allow",
    "*.env": "ask",          // .env 文件需要询问
  },
  bash: {
    "*": "ask",              // 所有 bash 命令都需要确认
    "git pull": "allow",     // 除外：git pull 允许
  },
})
```

#### 权限处理流程

```
┌─────────────────────────────────────────────────────────────────┐
│                    工具调用请求                                  │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  1. 检查权限规则                                                  │
│     - 是否被明确拒绝？                                            │
│     - 是否需要询问？                                              │
│     - 是否直接允许？                                              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  2. 需要询问 → 暂停，等待用户确认                                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  3. 执行工具                                                      │
│     - read: 读取文件内容                                         │
│     - write: 创建/修改文件                                       │
│     - bash: 执行命令                                             │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│  4. 返回结果                                                      │
│     - 成功：返回输出内容                                          │
│     - 失败：返回错误信息                                          │
└─────────────────────────────────────────────────────────────────┘
```

#### 为什么要这样做？

```
目的：安全第一，防止 AI 做出危险操作
```

- **渐进式信任**：一开始只给最小权限，随着用户确认逐步放开。
- **危险操作二次确认**：删除文件、执行 `rm -rf` 这种操作需要明确确认。
- **死循环检测**：如果 AI 在同一个操作上重复三次以上，主动询问用户。

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 安全性极高 | 可能打断 AI 的工作流程 |
| 用户完全控制 | 频繁的权限询问可能让用户厌烦 |
| 防止灾难性错误 | 配置复杂，需要仔细调优 |

---

### 阶段七：循环与终止

#### 发生了什么？

工具执行完成后，结果会被返回给 LLM。LLM 会根据结果决定：

1. **继续**：还需要更多信息或操作
2. **终止**：任务完成，给出最终回答

```typescript
// 处理器的主循环
while (true) {
  // 调用 LLM，获取响应
  const stream = await LLM.stream(streamInput)

  // 处理响应流
  for await (const value of stream.fullStream) {
    // ... 处理各种事件
  }

  // 检查是否需要终止
  if (blocked) return "stop"      // 被权限阻止
  if (error) return "stop"        // 发生错误
  if (needsCompaction) return "compact"  // 需要压缩历史
}
```

#### 完成标记

```typescript
case "finish-step":
  input.assistantMessage.finish = value.finishReason
  input.assistantMessage.cost += usage.cost  // 累加成本
  input.assistantMessage.tokens = usage.tokens
  break
```

#### 完整流程图

```
用户输入
    │
    ▼
┌─────────────────┐
│  构建 Prompt    │
└─────────────────┘
    │
    ▼
┌─────────────────┐
│  发送给 LLM      │
└─────────────────┘
    │
    ▼
┌─────────────────┐
│  LLM 响应       │
│  ┌───────────┐  │
│  │ 文本输出  │  │───► 显示给用户
│  └───────────┘  │
│  ┌───────────┐  │
│  │ 工具调用  │  │───► 执行工具
│  └───────────┘  │         │
│  ┌───────────┐  │         ▼
│  │ 推理过程  │  │   ┌───────────┐
│  └───────────┘  │   │ 返回结果  │
└─────────────────┘   └───────────┘
                           │
                           ▼
                    ┌───────────────┐
                    │ LLM 决定：    │
                    │ 继续 or 终止? │
                    └───────────────┘
                           │
              ┌────────────┴────────────┐
              ▼                         ▼
        继续循环                    任务完成
```

#### 优点和缺点

| 优点 | 缺点 |
|------|------|
| 灵活的循环机制 | 可能的无限循环 |
| 支持多轮对话 | 需要合理的终止条件 |
| 工具调用可以多次进行 | 成本会累积 |

---

## 三、核心设计模式

### 1. Provider 抽象层

```
┌─────────────────────────────────────────┐
│          OpenCode 应用层                │
└─────────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────┐
│          Provider 抽象层                │
│  - getLanguage(model)                   │
│  - streamText()                         │
└─────────────────────────────────────────┘
                   │
    ┌──────────────┼──────────────┐
    ▼              ▼              ▼
┌────────┐   ┌──────────┐   ┌──────────┐
│OpenAI  │   │Anthropic │   │ Google   │
└────────┘   └──────────┘   └──────────┘
```

**优点**：
- 切换模型只需改配置
- 新增 Provider 只需实现接口

**缺点**：
- 不同 Provider 的 API 有差异，需要适配

### 2. 权限继承模型

```
┌─────────────────────────────────────────────────────┐
│  默认规则 (defaults)                                 │
│  - read: allow                                      │
│  - bash: ask                                        │
└─────────────────────────────────────────────────────┘
                         +
┌─────────────────────────────────────────────────────┐
│  Agent 规则 (mode config)                           │
│  - plan: edit = deny                                │
└─────────────────────────────────────────────────────┘
                         +
┌─────────────────────────────────────────────────────┐
│  用户配置 (user config)                              │
│  - 可以覆盖以上规则                                  │
└─────────────────────────────────────────────────────┘
                         =
┌─────────────────────────────────────────────────────┐
│  最终生效的权限                                      │
└─────────────────────────────────────────────────────┘
```

### 3. 消息流设计

```
用户消息 ──► LLM ──► 工具调用 ──► 工具结果 ──► LLM ──► 最终响应
             │                                      │
             └──► 文本响应 ────────────────────────┘
```

---

## 四、完整示例：创建一个 Todo 组件

让我们用一个完整的例子，串联整个流程：

```
用户输入："帮我创建一个 TodoList 组件"
```

**阶段一：预处理**
- Agent: `build`（默认）
- 模型: `anthropic/claude-sonnet-4-20250514`

**阶段二：构建 Prompt**
```yaml
系统提示词: "你是一个专业的前端开发者，擅长 React、TypeScript..."

工具列表:
  - read: 读取文件
  - write: 创建文件
  - edit: 修改文件
  - bash: 执行命令

历史消息:
  - 用户：帮我创建一个 TodoList 组件
```

**阶段三：工具准备**
- 所有工具都可用（build Agent 有完整权限）
- 过滤后：46 个工具都传给 LLM

**阶段四：调用 LLM**
```
发送给 Claude:
[系统提示词]
[46个工具描述]
[用户消息]

Claude 回复:
"我来帮你创建一个 TodoList 组件。
首先让我查看一下项目的结构。"
```

**阶段五：处理响应**
- 显示文本："我来帮你创建一个 TodoList 组件..."
- 不需要工具调用，继续

**阶段六：LLM 决定继续**
```
Claude 回复:
"我看到这是一个 React 项目。
现在我将创建 TodoList 组件。

tool_call: {
  name: "write",
  args: {
    path: "src/components/TodoList.tsx",
    content: "..."
  }
}
```

**阶段七：执行工具**
1. 权限检查：build Agent 可以 write ✓
2. 执行 write 工具，创建文件
3. 返回结果：文件创建成功

**继续循环**
- LLM 收到文件创建成功的消息
- 决定再创建一个测试文件
- 再执行一个工具调用
- ...
- 最终，LLM 决定终止："组件已创建完成！"

---

## 五、关键要点总结

### 为什么这样设计？

1. **安全性优先**
   - 权限系统确保 AI 不会做出危险操作
   - 所有危险操作都需要用户确认

2. **灵活性**
   - 多 Agent 系统适配不同场景
   - 多 Provider 支持，随意切换模型

3. **可观测性**
   - 流式输出让用户实时看到进度
   - 工具调用过程透明可追踪

4. **可扩展性**
   - 插件系统可以注入自定义逻辑
   - 工具系统支持新增自定义工具

### 最佳实践

1. **合理选择 Agent**
   - 代码编写：用 `build`
   - 只读分析：用 `plan`
   - 代码探索：用 `explore`

2. **配置适当的权限**
   - 开发时：可以放宽权限提高效率
   - 敏感环境：严格限制危险操作

3. **理解提示词的作用**
   - 好的提示词能显著提升效果
   - 但提示词不是越长越好

---

*文档生成时间：2026-01-28*
